## 前言
前天，MetaAI发布了超大尺寸的开源模型——Llama3.1-405b，使用Ollama就可以部署，教程参照上上期博客，在cmd窗口输入部署命令即可
## 部署
| 模型尺寸      | 模型大小 |  部署命令  |
| ----------- | ----------- |----------- |
| llama3.1-8b      | 4.7GB       |  `ollama run llama3.1:8b`
| llama3.1-70b   |     40GB |    `ollama run llama3.1:70b`
| llama3.1-405b  | 231GB |`ollama run llama3.1:405b` |
## 官方描述
### Meta Llama 3.1
![4d0cab8e-952b-4c75-b110-1514d8db8fae](https://github.com/user-attachments/assets/2909eed4-fa28-4e65-a0c4-8758bb5a213d)
**Llama 3.1**模型系列可用： 
- **8B**
- **70B**
- **405B**

Llama 3.1 405B 是首个公开可用的模型，在常识、可操控性、数学、工具使用和多语言翻译等方面的先进能力上可与顶级人工智能模型相媲美。
8B 和 70B 模型的升级版本是多语言的，并且具有 128K 的显著更长的上下文长度、先进的工具使用能力和更强的整体推理能力。这使得 Meta 的最新模型能够支持高级用例，例如长篇文本摘要、多语言会话代理和编码助手。
Meta 还对其许可证进行了更改，允许开发人员使用 Llama 模型（包括 405B 模型）的输出以改进其他模型。

对于此次发布，Meta 已在涵盖多种语言的 150 多个基准数据集上评估了性能。此外，Meta 进行了广泛的人工评估，在现实场景中将 Llama 3.1 与竞争模型进行了比较。Meta 的实验评估表明，我们的旗舰模型在一系列任务上与领先的基础模型（包括 GPT-4、GPT-4o 和 Claude 3.5 Sonnet）具有竞争力。此外，Meta 的较小模型与具有相似参数数量的封闭和开放模型具有竞争力。 
#### 模型评估 
对于此次发布，Meta 已在超过 150 个涵盖多种语言的基准数据集上评估了性能。此外，Meta 进行了广泛的人工评估，在现实场景中将 Llama 3.1 与竞争模型进行了比较。Meta 的实验评估表明，我们的旗舰模型在包括 GPT-4、GPT-4o 和 Claude 3.5 Sonnet 在内的一系列任务中与领先的基础模型具有竞争力。此外，Meta 的较小模型与具有相似参数数量的封闭和开放模型具有竞争力。 
![73b11a5e-84e9-4397-9d47-f0299a6294b3](https://github.com/user-attachments/assets/77ba3ac9-9290-4cb3-ba87-858cabce9bee)


![ad042a1c-bbc7-47de-bbbf-78a3cfc13485](https://github.com/user-attachments/assets/e75105ca-ae37-46d3-a64b-e8c09ea3fa35)


![2d582df5-ce45-4326-85c5-254c917554b2](https://github.com/user-attachments/assets/52dc29e7-29dc-4a65-994b-34be420eef14)

#### 参考文献 

- [Meta AI Llama 3.1 launch blog post](https://ai.meta.com/blog/meta-llama-3-1/)

由**豆包**翻译

---
什么，你想说你要在自己电脑上部署405b，很遗憾，**你的电脑几乎不支持运转如此大规格的模型**，你可以继续玩上次部署的Qwen2
我问了问通义千问，这是它说的
Ask：模型尺寸为405b的开源模型需要怎样的电脑可以部署
Answer：对于一个拥有4050亿参数的大规模语言模型，如Meta即将发布的Llama 3-405B模型，部署这样的模型需要非常强大的计算资源。以下是一些推荐的硬件配置要求：

CPU
处理器: 高性能多核心处理器，例如Intel Xeon或AMD EPYC系列。这些处理器通常具有较多的核心数和较高的时钟速度，适合处理并行计算任务。
核心数量: 至少16核心，32核心或更多将更为理想。
缓存: 大量的三级缓存有助于提高性能。
GPU
显卡: 高端的专业级GPU，例如NVIDIA A100、H100或类似的高性能GPU。
显存: 至少32GB或更高，考虑到模型的大小，可能需要更大的显存。
CUDA核心数量: 越多越好，以支持并行处理能力。
内存 (RAM)
系统内存: 至少128GB或更高，以便能够存储模型和其他相关数据。
存储
SSD: 快速NVMe SSD用于操作系统和应用程序安装。
额外存储: 可能需要TB级别的存储空间来存放模型文件和其他相关数据。
其他组件
电源供应器 (PSU): 高功率且高效的电源供应器，至少850W以上。
冷却系统: 高效的散热解决方案，包括水冷系统。
主板: 支持所需CPU和GPU的高端主板，带有足够的PCI-E插槽。
网络连接
高速网络接口: 为了数据传输，可能需要10Gbps或更快的网络接口卡。
软件环境
操作系统: Linux发行版，比如Ubuntu Server，因为大多数深度学习框架都针对Linux进行了优化。
编程框架: PyTorch或其他支持大规模模型的框架。
驱动程序: 最新的GPU驱动程序以获得最佳性能。
请注意，**上述配置是非常高端的，并且成本极高。部署这样规模的模型通常是在数据中心或云环境中进行的，而不是个人电脑。如果是在本地部署，那么这将是一个相当昂贵的投资。**另外，由于模型的复杂性和计算需求，可能还需要分布式计算技术来分摊处理负载。
## 在线体验
虽然不能本地部署，但可以线上体验呀（都需要魔法）
### 方式1：优先使用[HuggingFace](https://huggingface.co/)
进入主页往下滑，找到“Trending on this week”→“Spaces”→“Llama3.1-405B”，点击后无需注册即可聊天，需要先调教下让它说中文。虽然加载进页面慢，但回复真的炒鸡快！
![屏幕截图_26-7-2024_17583_huggingface co](https://github.com/user-attachments/assets/20099c67-9d6d-4b76-9eeb-50913a27405f)
### 方式2：使用[MetaAI](https://www.meta.ai/)
注册后即可使用


